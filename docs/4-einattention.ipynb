{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## EinAttention demo\n",
    "\n",
    "EinAttention covers a significant number of efficient attention mechanisms, though certainly not all.\n",
    "\n",
    "There are just too many modifications to squeeze them into a single layer.\n",
    "\n",
    "Here is a one-line idea of EinAttention: provide a detailed description of attention (what-attends-where) in 2-3 lines of code.\n",
    "Patterns in EinAttention are significantly more complicated, so ... welcome to the hardest level of einops.\n",
    "\n",
    "Examples will tell the story better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange, reduce\n",
    "from einops.einattention import EinAttention\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1d cases\n",
    "\n",
    "generate some sample data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "batch, kv_tokens, channels = 7, 33, 55\n",
    "\n",
    "k, v = torch.from_numpy(np.random.RandomState(42).randn(2, batch, kv_tokens, channels))\n",
    "q_tokens = 22\n",
    "q = torch.from_numpy(np.random.RandomState(42).randn(batch, q_tokens, channels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanilla 1d attention\n",
    "\n",
    "Note: we only implement attention part, but do not touch a linear projection, and pre- or post- normalizations.\n",
    "\n",
    "\n",
    "\n",
    "Pattern:\n",
    "```\n",
    "output <- q_pattern, k_and_v_pattern  (order likely to change)\n",
    "```\n",
    "\n",
    "Star is a special embedding dimension, that is reduced and the recovered during attention.\n",
    "\n",
    "General recommendation to mark q-only and kv-only variables as such, though later I ignore this recommendation myself and mark kv only\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "att = EinAttention(\n",
    "    'b t_q * <- b t_q *, b t_kv *',\n",
    ")\n",
    "result = att(q, k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def btc2tbc(x):\n",
    "    return rearrange(x, 'b t c -> t b c')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's reorganize for sequence-first\n",
    "att = EinAttention(\n",
    "    't_q b * <- t_q b *, t_kv b *',\n",
    ")\n",
    "result_tbc = att(btc2tbc(q), btc2tbc(k), btc2tbc(v))\n",
    "# check that results are identical\n",
    "torch.allclose(result_tbc, btc2tbc(result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanilla 1d multi-head attention\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7, 22, 55])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_heads = 5\n",
    "att_mha = EinAttention(\n",
    "    'b t_q (head *) <- b t_q (head *), b t_kv (head *)', head=n_heads,\n",
    ")\n",
    "result = att_mha(q, k, v)\n",
    "result.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change first channel\n",
    "# and check that only first head is changed\n",
    "\n",
    "modified_q = q.clone()\n",
    "modified_q[:, :, 0] = 5\n",
    "number_of_differences = ~torch.isclose(\n",
    "    att_mha(q, k, v),\n",
    "    att_mha(modified_q, k, v)\n",
    ")\n",
    "reduce(number_of_differences.float(), 'b t (head c) -> head c', 'mean', head=n_heads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next examples I'll focus on non-multi-head versions, as multi-head attention is basically all about\n",
    "replacing * with (head, *)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Causal 1d attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class CausalAttentionBTC(EinAttention):\n",
    "    def __init__(self):\n",
    "        super().__init__('b t_q * <- b t_q *, b t_kv *')\n",
    "\n",
    "    def modify_logit_inplace(self, logit_all_dims):\n",
    "        t_q, t_kv = self.get_logit_dimensions_grid(logit_all_dims, ['t_q', 't_kv'])\n",
    "        causal_mask = (t_kv <= t_q)\n",
    "        logit_all_dims.add_(causal_mask.float().mul(100))\n",
    "\n",
    "att_causal = CausalAttentionBTC()\n",
    "\n",
    "# let's check it is still scriptable\n",
    "att_causal = torch.jit.script(att_causal)\n",
    "causal_result = att_causal(q, k, v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify q and check that only one output position changes\n",
    "q_modified = q.clone()\n",
    "q_modified[:, 6, :] = 0\n",
    "\n",
    "number_of_differences = ~torch.isclose(causal_result, att_causal(q_modified, k, v))\n",
    "reduce(number_of_differences.float(), 'b t c -> t', 'mean')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.9948,\n        1.0000, 1.0000, 1.0000, 0.9870, 0.9974, 1.0000, 1.0000, 1.0000, 1.0000,\n        1.0000, 1.0000, 1.0000, 1.0000])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify k and check that output changes only for positions 6 and after\n",
    "k_modified = k.clone()\n",
    "k_modified[:, 6, :] = 5\n",
    "\n",
    "number_of_differences = ~torch.isclose(causal_result, att_causal(q, k_modified, v))\n",
    "reduce(number_of_differences.float(), 'b t c -> t', 'mean', b=batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1.])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same exercise with v\n",
    "v_modified = v.clone()\n",
    "v_modified[:, 6, :] = 5\n",
    "\n",
    "number_of_differences = ~torch.isclose(causal_result, att_causal(q, k, v_modified))\n",
    "reduce(number_of_differences.float(), 'b t c -> t', 'mean', b=batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Skip some positions in attention\n",
    "\n",
    "To process sequences of different length within the same batch,\n",
    "a common recipe is pad_token.\n",
    "\n",
    "While that's quite ugly (see how we solve it in CAPE with variable stfts), that's a very common usecase, so here how EinAttention deals with it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# generate mask of arbitrary structure\n",
    "kv_masked = torch.from_numpy(np.random.RandomState(42).randn(batch, kv_tokens)) > 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "att_with_masks = EinAttention(\n",
    "    'b t_q * <- b t_q * , b t_kv *', logshift_forward='b t_kv'\n",
    ")\n",
    "att_with_masks = torch.jit.script(att_with_masks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "result_with_masks = att_with_masks(q, k, v, logshift=kv_masked.mul(-100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that change of masked elements does not have any effect\n",
    "k_modified = k.clone()\n",
    "k_modified[kv_masked] = -1\n",
    "\n",
    "v_modified = v.clone()\n",
    "v_modified[kv_masked] = -1\n",
    "\n",
    "torch.allclose(\n",
    "    att_with_masks(q, k, v, logshift=kv_masked.mul(-100)),\n",
    "    att_with_masks(q, k_modified, v_modified, logshift=kv_masked.mul(-100)),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now check that changes in non-masked elements affect result, modifying k\n",
    "k_modified = k.clone()\n",
    "k_modified[~kv_masked] = -1\n",
    "\n",
    "torch.allclose(\n",
    "    att_with_masks(q, k, v, logshift=kv_masked.mul(-100)),\n",
    "    att_with_masks(q, k_modified, v, logshift=kv_masked.mul(-100)),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now check that changes in non-masked elements affect result, modifying v\n",
    "v_modified = v.clone()\n",
    "v_modified[~kv_masked] = -1\n",
    "\n",
    "torch.allclose(\n",
    "    att_with_masks(q, k, v, logshift=kv_masked.mul(-100)),\n",
    "    att_with_masks(q, k, v_modified, logshift=kv_masked.mul(-100)),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-attention example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# say you have pairs of sentences and images; for every token in sentence you want to query over a corresponding image\n",
    "cross_attention = EinAttention(\n",
    "    'b t * <- b t *, b h w *'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7, 22, 11])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_attention(\n",
    "    q=torch.zeros(batch, q_tokens, 11),\n",
    "    k=torch.zeros(batch, 13, 17, 11),\n",
    "    v=torch.zeros(batch, 13, 17, 11),\n",
    ").shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2d attention\n",
    "\n",
    "Basically, vanilla ViT, but without multi-head"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "batch = 7\n",
    "height = width = 15 # 15 = 5 * 3, will be important later\n",
    "imchannels = 33\n",
    "\n",
    "q, k, v = torch.from_numpy(\n",
    "    np.random.RandomState(42).randn(3, batch, imchannels, height, width)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7, 33, 15, 15])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_self_2d = EinAttention(\n",
    "    'b * h w <- b * h w, b * h_kv w_kv',\n",
    ")\n",
    "attn_self_2d = torch.jit.script(attn_self_2d)\n",
    "attn_self_2d(q, k, v).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "modified_k = k.clone()\n",
    "# modify a single element, this will affect all output elements\n",
    "modified_k[:, :, 0, 0] = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0004)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(\n",
    "    attn_self_2d(q, k, v),\n",
    "    attn_self_2d(q, modified_k, v),\n",
    ").float().mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Windowed attention (like SWIN transformers, but shifts are skipped)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7, 33, 15, 15])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_windows = 5\n",
    "\n",
    "attn_windows = EinAttention(\n",
    "    'b * (Wh h) (Ww w) <- b * (Wh h) (Ww w), b * (Wh h_kv) (Ww w_kv)',\n",
    "    Wh=n_windows, Ww=n_windows,\n",
    ")\n",
    "\n",
    "attn_windows(q, k, v).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x160cb9640>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAMMklEQVR4nO3df+xd9V3H8edLCk4YgSKMMX4IGEKCiwm0IWwucxHFDgndH/ujZNMylpDFTMEsWYokLvGvzZn5Iy4uBKYYG1hk4MgCjsq2GBOpK7X8KGXQYQVqocUZ2NwfrPHtH/d0+e7r/bblnnNu7/g8H8nNPfeez/d73j33++r5cU/OO1WFpDe/nzrWBUiaD8MuNcKwS40w7FIjDLvUiFXzXFiSmU79r1mzZuhSpDelPXv28Morr2TavLmGfVbbtm071iVIPxHWrl274jx346VGGHapEb3CnmRdkm8n2Z1k01BFSRrezGFPchzweeD9wCXAdUkuGaowScPqs2W/HNhdVc9V1evA3cD6YcqSNLQ+YT8beGHJ6xe79yQtoNG/ektyI3Dj2MuRdHh9wr4XOHfJ63O6935MVd0G3AazX1Qjqb8+u/HfAi5KckGSE4ANwP3DlCVpaDNv2avqYJKPA18DjgO+WFU7B6tM0qB6HbNX1QPAAwPVImlEXkEnNcKwS42Ya9jXrFlDVb3hh6T+3LJLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjejT6+3cJN9I8lSSnUluGrIwScPqc3fZg8Anqmp7kpOBR5NsqaqnBqpN0oBm3rJX1b6q2t5Nfw/Yhb3epIU1yDF7kvOBS4GtQ/w+ScPrHfYkbwW+DNxcVa9NmX9jkm1Jth04cKDv4iTNqFfYkxzPJOibq+reaWOq6raqWltVa88444w+i5PUQ5+z8QHuAHZV1eeGK0nSGPps2X8J+E3gV5Ls6B5XD1SXpIH16eL6z0AGrEXSiLyCTmqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRFDNIk4Lsm/JfnqEAVJGscQW/abmPR5k7TA+naEOQf4DeD2YcqRNJa+W/Y/BT4J/G//UiSNqU/7p2uA/VX16BHG2dhRWgB92z9dm2QPcDeTNlB/u3yQjR2lxTBz2Kvqlqo6p6rOBzYAX6+qDw9WmaRB+T271IiZGzsuVVXfBL45xO+SNA637FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS43o2/7p1CT3JHk6ya4k7xqqMEnD6nt32T8D/qGqPpjkBODEAWqSNIKZw57kFOC9wPUAVfU68PowZUkaWp/d+AuAA8Bfdf3Zb09y0kB1SRpYn7CvAi4D/rKqLgX+B9i0fJCNHaXF0CfsLwIvVtXW7vU9TML/Y2zsKC2GPo0dXwJeSHJx99aVwFODVCVpcH3Pxv8OsLk7E/8c8JH+JUkaQ6+wV9UOYO0wpUgak1fQSY0w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNaJvY8ffS7IzyZNJ7krylqEKkzSsmcOe5Gzgd4G1VfVO4Dhgw1CFSRpW3934VcDPJFnFpIPrf/YvSdIY+nSE2Qv8MfA8sA94taoeGqowScPqsxu/GljPpJvrO4CTknx4yjgbO0oLoM9u/K8C/15VB6rqh8C9wLuXD7Kxo7QY+oT9eeCKJCcmCZPGjruGKUvS0Pocs29l0qZ5O/BE97tuG6guSQPr29jxU8CnBqpF0oi8gk5qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRRwx7ki8m2Z/kySXvnZZkS5Jnu+fV45Ypqa+j2bL/NbBu2XubgIer6iLg4e61pAV2xLBX1T8B31329nrgzm76TuADw5YlaWizHrOfWVX7uumXgDMHqkfSSHqfoKuqAmql+TZ2lBbDrGF/OclZAN3z/pUG2thRWgyzhv1+YGM3vRH4yjDlSBrL0Xz1dhfwL8DFSV5M8lHg08CvJXmWSevmT49bpqS+jtjYsaquW2HWlQPXImlEXkEnNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUiFkbO342ydNJHk9yX5JTR61SUm+zNnbcAryzqn4ReAa4ZeC6JA1spsaOVfVQVR3sXj4CnDNCbZIGNMQx+w3AgwP8Hkkj6hX2JLcCB4HNhxljY0dpAcwc9iTXA9cAH+o6uU5lY0dpMRyx/dM0SdYBnwR+uap+MGxJksYwa2PHvwBOBrYk2ZHkCyPXKamnWRs73jFCLZJG5BV0UiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjZipseOSeZ9IUklOH6c8SUOZtbEjSc4FrgKeH7gmSSOYqbFj50+YNIpYsRuMpMUx0zF7kvXA3qp6bOB6JI3kDbd/SnIi8PtMduGPZvyNwI0A55133htdnKSBzLJl/3ngAuCxJHuY9GbfnuTt0wbb2FFaDG94y15VTwBvO/S6C/zaqnplwLokDWzWxo6SfsLM2thx6fzzB6tG0mi8gk5qhGGXGpGq+V0Tk+QA8B8rzD4dWKSTfItWDyxeTdZzeMeinp+rqqlfe8017IeTZFtVrT3WdRyyaPXA4tVkPYe3aPW4Gy81wrBLjViksN92rAtYZtHqgcWryXoOb6HqWZhjdknjWqQtu6QRGXapEXMPe5J1Sb6dZHeSTVPm/3SSL3XztyY5f8Razk3yjSRPJdmZ5KYpY96X5NUkO7rHH4xVz5Jl7knyRLe8bVPmJ8mfd+vo8SSXjVjLxUv+7TuSvJbk5mVjRl1H026NluS0JFuSPNs9r17hZzd2Y55NsnHEej6b5Onu87gvyakr/OxhP9tRVdXcHsBxwHeAC4ETgMeAS5aN+W3gC930BuBLI9ZzFnBZN30y8MyUet4HfHXO62kPcPph5l8NPAgEuALYOsfP7yUmF27MbR0B7wUuA55c8t4fAZu66U3AZ6b83GnAc93z6m569Uj1XAWs6qY/M62eo/lsx3zMe8t+ObC7qp6rqteBu4H1y8asB+7spu8BrkySMYqpqn1Vtb2b/h6wCzh7jGUNbD3wNzXxCHBqkrPmsNwrge9U1UpXQY6ipt8abenfyZ3AB6b86K8DW6rqu1X138AWptxPcYh6quqhqjrYvXyEyX0eFsq8w3428MKS1y/y/8P1ozHdynsV+NmxC+sOFy4Ftk6Z/a4kjyV5MMkvjF0Lk/v6PZTk0e5OP8sdzXocwwbgrhXmzXsdnVlV+7rpl4Azp4w5VuvpBiZ7XtMc6bMdzRu+ecWbUZK3Al8Gbq6q15bN3s5kt/X7Sa4G/h64aOSS3lNVe5O8DdiS5Olua3LMJDkBuBa4ZcrsY7GOfqSqKslCfIec5FbgILB5hSHH7LOd95Z9L3DuktfndO9NHZNkFXAK8F9jFZTkeCZB31xV9y6fX1WvVdX3u+kHgOPHvk9+Ve3tnvcD9zE5/FnqaNbj0N4PbK+ql5fPOBbrCHj50KFL97x/ypi5rqck1wPXAB+q7gB9uaP4bEcz77B/C7goyQXdlmIDcP+yMfcDh86afhD4+korrq/uXMAdwK6q+twKY95+6JxBksuZrLMx//M5KcnJh6aZnPhZ3qDjfuC3urPyVwCvLtmlHct1rLALP+911Fn6d7IR+MqUMV8Drkqyujtbf1X33uCSrGNya/Vrq+oHK4w5ms92PPM+I8jkTPIzTM7K39q994dMVhLAW4C/A3YD/wpcOGIt72FyDPU4sKN7XA18DPhYN+bjwE4m3xw8Arx75PVzYbesx7rlHlpHS2sK8PluHT7B5B6AY9Z0EpPwnrLkvbmtIyb/yewDfsjkuPujTM7jPAw8C/wjcFo3di1w+5KfvaH7W9oNfGTEenYzOT9w6O/o0DdK7wAeONxnO6+Hl8tKjfAKOqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGvF/DSDRxSpo/IgAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_modified = q.clone()\n",
    "# modify top-left corner\n",
    "# and check that only one position changed in the output\n",
    "q_modified[:, :, 0, 0] = 5\n",
    "number_of_differences = torch.isclose(\n",
    "    attn_windows(q, k, v),\n",
    "    attn_windows(q_modified, k, v),\n",
    ")\n",
    "plt.imshow(\n",
    "    reduce(number_of_differences.float(), 'b c h w -> h w', 'mean'),\n",
    "    cmap='gray'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x160db4940>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_modified = k.clone()\n",
    "# modify top-left corner\n",
    "# and check that only positions in a window changed\n",
    "k_modified[:, :, 0, 0] = 5\n",
    "number_of_differences = torch.isclose(\n",
    "    attn_windows(q, k, v),\n",
    "    attn_windows(q, k_modified, v),\n",
    ")\n",
    "plt.imshow(\n",
    "    reduce(number_of_differences.float(), 'b c h w -> h w', 'mean'),\n",
    "    cmap='gray'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Windowed attention with logit shifts\n",
    "\n",
    "To provide some information about positions of elements in attention, there are several ways:\n",
    "\n",
    "1. add a learnable term to logits, cheap and seems to work\n",
    "2. add position-dependent term to q / k / v or only to q / k; this probably worth adding, but it can be done as well outside of attention module.\n",
    "3. all kinds of relpos. This group is messy, implementations are hard and frequently resource-hungry specially in some corner cases.\n",
    "   - I am not convinced this group even worth thinking about - see again CAPE, benefits of relpos are minimal, headache is maximal\n",
    "\n",
    "So, let's add option 1. to previous method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7, 33, 15, 15])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3\n",
    "\n",
    "attn_windows = EinAttention(\n",
    "    'b * (Wh h) (Ww w) <- b * (Wh h) (Ww w), b * (Wh h_kv) (Ww w_kv)',\n",
    "    # which parameters define shift? internally a tensor of this shape is created\n",
    "    logshift_param=\"h w h_kv w_kv\",\n",
    "    # now we need to specify all the dimension so that this tensor could be created\n",
    "    h=window_size, w=window_size, h_kv=window_size, w_kv=window_size,\n",
    ")\n",
    "\n",
    "attn_windows(q, k, v).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Strided attention (MaxVit / MobileViT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 33, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "stride = 3\n",
    "\n",
    "attn_strides = EinAttention(\n",
    "    'b * (Wh h) (Ww w) <- b * (Wh h) (Ww w), b * (Wh_kv h) (Ww_kv w)',\n",
    "    h=stride, w=stride,\n",
    ")\n",
    "\n",
    "print(\n",
    "    attn_strides(q, k, v).shape\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x160e33880>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_differences = torch.isclose(\n",
    "    attn_strides(q, k, v),\n",
    "    attn_strides(q, k_modified, v),\n",
    ")\n",
    "plt.imshow(\n",
    "    reduce(number_of_differences.float(), 'b c h w -> h w', 'mean'),\n",
    "    cmap='gray'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frenzy\n",
    "\n",
    "let's make an attention layer for video.\n",
    "\n",
    "- Multi headed, but also with groups of heads.\n",
    "- strided on time and image dimensions\n",
    "- unless you have H100 gpu, we can skip batch dimensions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class CausalVideoAttention(EinAttention):\n",
    "    def __init__(self, stride: int, time_stride: int, n_heads: int):\n",
    "        super().__init__(\n",
    "            '(T t) (head *) (H h) (W w) <- (T t) (head *) (H h) (W w), (T_kv t) (head *) (H_kv h) (W_kv w)',\n",
    "            h=stride, w=stride, t=time_stride, head=n_heads,\n",
    "        )\n",
    "\n",
    "    def modify_logit_inplace(self, logit_all_dims):\n",
    "        T_q, T_kv = self.get_logit_dimensions_grid(logit_all_dims, ['T', 'T_kv'])\n",
    "        causal_mask = (T_kv <= T_q)\n",
    "        logit_all_dims.add_(causal_mask.float().mul(100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "CausalVideoAttention.get_logit_dimensions_grid??"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([18, 22, 15, 20])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoatt = CausalVideoAttention(time_stride=3, stride=5, n_heads=2)\n",
    "x = torch.randn(6 * 3, 2 * 11, 3 * 5, 4 * 5) # lazy, will use a single tensor\n",
    "videoatt(q=x, k=x, v=x).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flexible logshift-based learnable relpos for 1d\n",
    "\n",
    "Built-in logshift_param has limited flexibility: corresponding dimensions should be fixed.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "Einattention(...)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose we want to introduce a logshift-based learned attention (because we have learnable relpos) to vanilla 1d self-attention\n",
    "\n",
    "EinAttention('b t_q * <- b t_q *, b t_kv *')\n",
    "\n",
    "# if we introduce logshift_param like this, it is a full matrix and all sequences passed should be of the same length\n",
    "# - for most applications that's a no-go.\n",
    "EinAttention(\n",
    "    'b t_q (head *) <- b t_q (head *), b t_kv (head *)',\n",
    "    logshift_param='head t_q t_kv',\n",
    "    t_q=100, t_kv=100, head=8,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# So let's implement a custom relshift, again by subclassing\n",
    "# and defining custom logit modification\n",
    "\n",
    "class RelPosWithLearnableLogshifts(EinAttention):\n",
    "    def __init__(self, relpos_length: int, n_heads):\n",
    "        super().__init__('b t_q (h *) <- b t_q (h *), b t_kv (h *)', h=n_heads)\n",
    "\n",
    "        self.shifts = torch.nn.Parameter(torch.zeros(n_heads, 2 * relpos_length - 1).float())\n",
    "        self.relpos_length = relpos_length\n",
    "\n",
    "    def modify_logit_inplace(self, logit_all_dims):\n",
    "        head, t_q, t_kv = self.get_logit_dimensions_grid(logit_all_dims, ['h', 't_q', 't_kv'])\n",
    "        # clip maximal distance. Note that here we rely on negative indexing (this can be changed)\n",
    "        distance = (t_q - t_kv).clamp(-self.relpos_length, self.relpos_length)\n",
    "\n",
    "        shifts = self.shifts[head, distance]\n",
    "        logit_all_dims.add_(shifts)\n",
    "\n",
    "        # we can additionally mask for causality here if we want."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 25, 12])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relpos = RelPosWithLearnableLogshifts(relpos_length=11, n_heads=4)\n",
    "q, k, v = torch.randn(3, 5, 25, 4 * 3)\n",
    "relpos(q, k, v).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}